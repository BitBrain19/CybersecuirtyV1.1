"""
FIXED Vulnerability Assessment Model - Production Ready
========================================================

Key Improvements:
1. Proper ensemble model initialization and training
2. Fixed preprocessing pipeline with correct features
3. Type-safe model output
4. Comprehensive input validation
5. Thread resource cleanup
6. Better error handling
7. Consistent severity classification
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import (
    GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor
)
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import IsolationForest
import joblib
import os
import mlflow
import time
import hashlib
from typing import Dict, Any, Tuple, Optional, List
from dataclasses import dataclass, field
from datetime import datetime
from loguru import logger
from threading import RLock
import atexit

from ..core.exceptions import (
    ModelError, DataValidationError, PredictionError,
    retry_with_backoff, create_error_context
)
from ..core.monitoring import metrics_collector
from ..core.config import settings


@dataclass
class VulnerabilityMetadata:
    """Metadata for vulnerability assessment model."""
    version: str = "2.1"
    training_samples: int = 0
    feature_count: int = 0
    last_trained: Optional[float] = None
    model_hash: Optional[str] = None
    performance_metrics: Dict[str, float] = field(default_factory=dict)


class VulnerabilityAssessmentModel:
    """Production-grade vulnerability assessment model.
    
    FIXED:
    - All ensemble models trained (not just primary)
    - Proper feature engineering
    - Type-safe output
    - Thread-safe operations with cleanup
    """
    
    def __init__(self, model_path: Optional[str] = None, 
                 enable_ensemble: bool = True,
                 enable_anomaly_detection: bool = True):
        """Initialize the vulnerability assessment model."""
        self.random_state = 42
        self.enable_ensemble = enable_ensemble
        self.enable_anomaly_detection = enable_anomaly_detection
        
        self._lock = RLock()
        self.model_metadata = VulnerabilityMetadata()
        
        # FIX: Initialize all ensemble models
        self.models = self._initialize_models()
        
        self.preprocessor = None
        self.feature_selector = None
        self.scaler = RobustScaler()
        self.scaler_fit_state = False
        
        # Anomaly detection
        self.anomaly_detector = (
            IsolationForest(contamination=0.1, random_state=42)
            if enable_anomaly_detection else None
        )
        
        # Feature definitions
        self.numerical_features = [
            'age', 'version', 'patch_level', 'complexity_score',
            'exposure_score', 'exploitability_score', 'impact_score',
            'cvss_base_score'
        ]
        
        self.categorical_features = [
            'os_type', 'service_type', 'access_vector', 'authentication',
            'vulnerability_type', 'vendor'
        ]
        
        self.expected_features = self.numerical_features + self.categorical_features
        
        # Severity thresholds
        self.severity_thresholds = {
            'low': (0.0, 3.0),
            'medium': (3.0, 7.0),
            'high': (7.0, 9.0),
            'critical': (9.0, 10.0)
        }
        
        # Performance tracking
        self._prediction_count = 0
        self._last_prediction_time = None
        
        # Initialize preprocessing
        self._initialize_preprocessor()
        
        # Load model if provided
        if model_path and os.path.exists(model_path):
            self.load(model_path)
        
        # Register cleanup
        atexit.register(self.cleanup)
        logger.info("VulnerabilityAssessmentModel initialized")
    
    def _initialize_models(self) -> Dict[str, Any]:
        """FIX: Initialize all ensemble models properly."""
        models = {
            'primary': GradientBoostingRegressor(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=6,
                random_state=self.random_state
            )
        }
        
        if self.enable_ensemble:
            models.update({
                'random_forest': RandomForestRegressor(
                    n_estimators=150,
                    max_depth=10,
                    random_state=self.random_state,
                    n_jobs=-1
                ),
                'extra_trees': ExtraTreesRegressor(
                    n_estimators=100,
                    max_depth=8,
                    random_state=self.random_state,
                    n_jobs=-1
                ),
                'ridge': Ridge(alpha=1.0, random_state=self.random_state)
            })
        
        return models
    
    def _initialize_preprocessor(self):
        """Initialize the preprocessing pipeline."""
        try:
            numerical_transformer = Pipeline([
                ('scaler', RobustScaler())
            ])
            
            categorical_transformer = Pipeline([
                ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
            ])
            
            self.preprocessor = ColumnTransformer(
                transformers=[
                    ('num', numerical_transformer, self.numerical_features),
                    ('cat', categorical_transformer, self.categorical_features)
                ],
                remainder='drop'
            )
            
            logger.info(f"Preprocessor initialized for {len(self.expected_features)} features")
            
        except Exception as e:
            logger.error(f"Preprocessor initialization failed: {e}")
            raise
    
    @retry_with_backoff(max_retries=2)
    def preprocess_features(self, features: Dict[str, Any]) -> np.ndarray:
        """Preprocess features with validation.
        
        FIXED:
        - Validates all inputs
        - Handles missing features properly
        - Never re-fits scaler during inference
        """
        try:
            start_time = time.time()
            
            # Validate inputs
            self._validate_input_features(features)
            
            # Convert to DataFrame
            df = pd.DataFrame([features])
            
            # Fill missing features
            df = self._handle_missing_features(df)
            
            # Handle outliers
            df = self._handle_outliers(df)
            
            # Apply preprocessing
            if self.preprocessor is None:
                raise ModelError(
                    "vulnerability_assessment",
                    "Preprocessor not initialized"
                )
            
            if not self.scaler_fit_state:
                # During training setup
                processed = self.preprocessor.fit_transform(df)
                self.scaler_fit_state = True
            else:
                # During inference - never refit!
                processed = self.preprocessor.transform(df)
            
            # Feature selection
            if self.feature_selector is not None:
                processed = self.feature_selector.transform(processed)
            
            processing_time = time.time() - start_time
            metrics_collector.record_metric(
                "vulnerability_preprocessing_time",
                processing_time,
                {"model": "vulnerability_assessment"}
            )
            
            logger.debug(f"Features preprocessed in {processing_time:.3f}s")
            return processed
            
        except Exception as e:
            context = create_error_context(
                model_name="vulnerability_assessment",
                endpoint="preprocess_features"
            )
            raise DataValidationError(
                f"Preprocessing failed: {str(e)}",
                context=context
            )
    
    def _validate_input_features(self, features: Dict[str, Any]):
        """Validate input features."""
        if not features:
            raise DataValidationError("Empty features dictionary")
        
        # Check numerical features are numeric
        for feature in self.numerical_features:
            if feature in features and features[feature] is not None:
                try:
                    float(features[feature])
                except (ValueError, TypeError):
                    raise DataValidationError(
                        f"Feature '{feature}' must be numeric"
                    )
    
    def _handle_missing_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Fill missing features with defaults."""
        for feature in self.numerical_features:
            if feature not in df.columns:
                df[feature] = 0.0
        
        for feature in self.categorical_features:
            if feature not in df.columns:
                df[feature] = 'unknown'
        
        return df
    
    def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Detect and handle outliers."""
        if self.anomaly_detector is not None:
            try:
                numerical_data = df[self.numerical_features].fillna(0)
                outlier_mask = self.anomaly_detector.predict(numerical_data) == -1
                
                if outlier_mask. any():
                    logger.warning(f"Detected {outlier_mask.sum()} outliers")
                    
            except Exception as e:
                logger.warning(f"Outlier detection failed: {e}")
        
        return df
    
    @retry_with_backoff(max_retries=2)
    def train(self, X: pd.DataFrame, y: np.ndarray,
              validation_split: float = 0.2,
              optimize_hyperparameters: bool = True) -> Dict[str, Any]:
        """Train all ensemble models.
        
        FIXED:
        - All models trained (not just primary)
        - Proper hyperparameter optimization
        - Comprehensive validation
        """
        start_time = time.time()
        
        try:
            with self._lock:
                logger.info(f"Starting training with {len(X)} samples")
                
                # Validate training data
                self._validate_training_data(X, y)
                
                # Split data
                X_train, X_val, y_train, y_val = train_test_split(
                    X, y, test_size=validation_split,
                    random_state=self.random_state
                )
                
                # Reset scaler state
                self.scaler_fit_state = False
                
                # Preprocess training data
                X_train_proc = self.preprocessor.fit_transform(X_train)
                X_val_proc = self.preprocessor.transform(X_val)
                self.scaler_fit_state = True
                
                # Train anomaly detector
                if self.anomaly_detector is not None:
                    self.anomaly_detector.fit(X_train_proc)
                
                training_results = {}
                
                # FIX: Train ALL models
                for model_name, model in self.models.items():
                    logger.info(f"Training {model_name}...")
                    
                    # Hyperparameter optimization for primary model
                    if model_name == 'primary' and optimize_hyperparameters:
                        model = self._optimize_hyperparameters(
                            model, X_train_proc, y_train
                        )
                        self.models[model_name] = model
                    
                    # Train model
                    model.fit(X_train_proc, y_train)
                    
                    # Evaluate
                    y_pred = model.predict(X_val_proc)
                    mse = mean_squared_error(y_val, y_pred)
                    mae = mean_absolute_error(y_val, y_pred)
                    r2 = r2_score(y_val, y_pred)
                    
                    training_results[model_name] = {
                        'mse': mse,
                        'mae': mae,
                        'r2': r2
                    }
                    
                    logger.info(f"{model_name}: MSE={mse:.4f}, RÂ²={r2:.4f}")
                
                # Cross-validation
                cv_scores = cross_val_score(
                    self.models['primary'], X_train_proc, y_train,
                    cv=5, scoring='neg_mean_squared_error'
                )
                training_results['cv_mse_mean'] = -cv_scores.mean()
                training_results['cv_mse_std'] = cv_scores.std()
                
                # Update metadata
                training_time = time.time() - start_time
                self.model_metadata.training_samples = len(X)
                self.model_metadata.feature_count = X_train_proc.shape[1]
                self.model_metadata.last_trained = time.time()
                self.model_metadata.performance_metrics = {
                    'best_r2': max(r['r2'] for r in training_results.values() if isinstance(r, dict))
                }
                
                logger.info(f"Training completed in {training_time:.2f}s")
                
                return {
                    'training_results': training_results,
                    'training_time': training_time,
                    'metadata': self.model_metadata
                }
                
        except Exception as e:
            print(f"DEBUG: Training failed with error: {e}")
            import traceback
            traceback.print_exc()
            context = create_error_context(
                model_name="vulnerability_assessment",
                endpoint="train"
            )
            raise ModelError(
                "vulnerability_assessment",
                f"Training failed: {str(e)}",
                context=context
            )
    
    def _validate_training_data(self, X: pd.DataFrame, y: np.ndarray):
        """Validate training data."""
        if X is None or len(X) == 0:
            raise DataValidationError("Empty training data")
        
        if y is None or len(y) == 0:
            raise DataValidationError("Empty training labels")
        
        if len(X) != len(y):
            raise DataValidationError("Feature/label length mismatch")
        
        if len(X) < 20:
            logger.warning(f"Small dataset: {len(X)} samples")
        
        # Check vulnerability scores are in valid range
        if not all(0 <= score <= 10 for score in y):
            raise DataValidationError("Vulnerability scores must be in [0, 10]")
    
    def _optimize_hyperparameters(self, model, X_train: np.ndarray, y_train: np.ndarray):
        """Optimize hyperparameters for Gradient Boosting."""
        try:
            param_grid = {
                'n_estimators': [100, 200],
                'learning_rate': [0.05, 0.1],
                'max_depth': [4, 6, 8],
            }
            
            grid_search = GridSearchCV(
                GradientBoostingRegressor(random_state=self.random_state),
                param_grid,
                cv=3,
                scoring='neg_mean_squared_error',
                n_jobs=-1
            )
            
            grid_search.fit(X_train, y_train)
            logger.info(f"Best hyperparameters: {grid_search.best_params_}")
            
            return grid_search.best_estimator_
            
        except Exception as e:
            logger.warning(f"Hyperparameter optimization failed: {e}")
            return model
    
    @retry_with_backoff(max_retries=2)
    def predict(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """Make a prediction.
        
        FIXED:
        - Type-safe predictions
        - Proper ensemble voting
        - Consistent output format
        """
        start_time = time.time()
        
        try:
            # Check models exist
            if not any(self.models.values()):
                raise ModelError(
                    "vulnerability_assessment",
                    "No trained models available"
                )
            
            # Preprocess
            X = self.preprocess_features(features)
            
            # Get ensemble predictions
            ensemble_preds = self._get_ensemble_predictions(X)
            
            # Combine predictions
            final_score = self._combine_predictions(ensemble_preds)
            final_score = float(np.clip(final_score, 0.0, 10.0))
            
            # Determine severity
            severity = self._get_severity(final_score)
            
            # Detect anomaly
            is_anomaly = self._detect_anomaly(X)
            
            # Generate metadata
            metadata = {
                'ensemble_predictions': ensemble_preds,
                'is_anomaly': is_anomaly,
                'processing_time_ms': (time.time() - start_time) * 1000,
                'model_version': self.model_metadata.version
            }
            
            self._prediction_count += 1
            self._last_prediction_time = time.time()
            
            return {
                'vulnerability_score': final_score,
                'severity': severity,
                'is_anomaly': is_anomaly,
                'ensemble_predictions': ensemble_preds,
                'metadata': metadata
            }
            
        except Exception as e:
            context = create_error_context(
                model_name="vulnerability_assessment",
                endpoint="predict"
            )
            raise PredictionError(
                f"Prediction failed: {str(e)}",
                context=context
            )
    
    def _get_ensemble_predictions(self, X: np.ndarray) -> Dict[str, float]:
        """Get predictions from all models."""
        predictions = {}
        
        for model_name, model in self.models.items():
            try:
                if model is not None:
                    pred = float(model.predict(X)[0])
                    predictions[model_name] = np.clip(pred, 0.0, 10.0)
            except Exception as e:
                logger.warning(f"Prediction failed for {model_name}: {e}")
        
        return predictions
    
    def _combine_predictions(self, predictions: Dict[str, float]) -> float:
        """Combine ensemble predictions with weighted averaging."""
        if not predictions:
            return 5.0  # Default middle score
        
        # Weights for ensemble models
        weights = {
            'primary': 0.4,
            'random_forest': 0.3,
            'extra_trees': 0.2,
            'ridge': 0.1
        }
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for model_name, prediction in predictions.items():
            weight = weights.get(model_name, 0.1)
            weighted_sum += prediction * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 5.0
    
    def _get_severity(self, score: float) -> str:
        """Convert score to severity level."""
        for severity, (min_val, max_val) in self.severity_thresholds.items():
            if min_val <= score < max_val:
                return severity
        return 'critical'
    
    def _detect_anomaly(self, X: np.ndarray) -> bool:
        """Detect if input is anomalous."""
        if self.anomaly_detector is None:
            return False
        
        try:
            prediction = self.anomaly_detector.predict(X)[0]
            return bool(prediction == -1)
        except Exception:
            return False
    
    def cleanup(self):
        """Cleanup resources."""
        logger.info("Cleaning up VulnerabilityAssessmentModel resources")
        self.models.clear()
        if self.anomaly_detector is not None:
            self.anomaly_detector = None
    
    def save(self, path: str) -> None:
        """Save model to file."""
        os.makedirs(os.path.dirname(path) or '.', exist_ok=True)
        joblib.dump({
            'models': self.models,
            'preprocessor': self.preprocessor,
            'scaler': self.scaler,
            'anomaly_detector': self.anomaly_detector,
            'metadata': self.model_metadata,
            'severity_thresholds': self.severity_thresholds
        }, path)
        logger.info(f"Model saved to {path}")
    
    def load(self, path: str) -> None:
        """Load model from file."""
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model not found: {path}")
        
        data = joblib.load(path)
        self.models = data['models']
        self.preprocessor = data['preprocessor']
        self.scaler = data['scaler']
        self.anomaly_detector = data['anomaly_detector']
        self.model_metadata = data['metadata']
        self.severity_thresholds = data['severity_thresholds']
        self.scaler_fit_state = True
        logger.info(f"Model loaded from {path}")
